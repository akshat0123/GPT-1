checkpoint: 'data/checkpoints/model/koala'
epochs: 100

window_size: &window_size 128
vocab_size: &vocab_size 30915
device: &device 'cuda:0'
unk: &unk 30914

train_data:
    datapath: 'data/model/train.txt'
    window_size: *window_size
    vocab_size: *vocab_size
    unk: *unk

dev_data:
    datapath: 'data/model/dev.txt'
    window_size: *window_size
    vocab_size: *vocab_size
    unk: *unk

model:
    vocab_size: *vocab_size
    seq_size: *window_size
    n_layers: 12
    d_emb: 768
    d_k: 64
    d_v: 64
    d_h: 3072
    n_heads: 12
    dropout: 0.1
    device: *device

opt:
    lr: 0.00025
    betas: [0.9, 0.95]

sch:
    max_lr: 0.00025
    total_steps: 250000
    anneal_strategy: 'cos'
    pct_start: 0.10

trainer:
    device: *device

train_subset:
    size: 80000

dev_subset:
    size: 8000

loader:
    drop_last: True
    batch_size: 32

logger:
    logdir: 'runs/koala'
